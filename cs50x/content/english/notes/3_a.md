Lecture 3
=========

*   [Searching](#searching)
*   [Big O](#big-o)
*   [Linear search](#linear-search)
*   [Structs](#structs)
*   [Sorting](#sorting)
*   [Selection sort](#selection-sort)
*   [Recursion](#recursion)
*   [Merge sort](#merge-sort)

Searching
---------

*   Last time, we talked about memory in a computer, or RAM, and how our data can be stored as individual variables or as arrays of many items, or elements.
*   We can think of an array with a number of items as a row of lockers, where a computer can only open one locker to look at an item, one at a time.
*   For example, if we want to check whether a number is in an array, with an algorithm that took in an array as input and produce a boolean as a result, we might:
    *   look in each locker, or at each element, one at a time, from the beginning to the end.
        *   This is called **linear search**, where we move in a line, since our array isn’t sorted.
    *   start in the middle and move left or right depending on what we’re looking for, if our array of items is sorted.
        *   This is called **binary search**, since we can divide our problem in two with each step, like what David did with the phone book in week 0.
*   We might write pseudocode for linear search with:
    
        For i from 0 to n–1
            If i'th element is 50
                Return true
        Return false
        
    
    *   We can label each of `n` lockers from `0` to `n–1`, and check each of them in order.
*   For binary search, our algorithm might look like:
    
        If no items
            Return false
        If middle item is 50
            Return true
        Else if 50 < middle item
            Search left half
        Else if 50 > middle item
            Search right half
        
    
    *   Eventually, we won’t have any parts of the array left (if the item we want wasn’t in it), so we can return `false`.
    *   Otherwise, we can search each half depending on the value of the middle item.

Big O
-----

*   In week 0, we saw different types of algorithms and their running times: ![chart with: "size of problem" as x–axis; "time to solve" as y–axis; red, steep straight line from origin to top of graph labeled "n"; yellow, less steep straight line from origin to top of graph labeled "n/2"; green, curved line that gets less and less steep from origin to right of graph labeled "log_2 n"](https://cs50.harvard.edu/x/2020/notes/3/running_time.png)
*   The more formal way to describe this is with big _O_ notation, which we can think of as “on the order of”. For example, if our algorithm is linear search, it will take approximately _O_(_n_) steps, “on the order of _n_”. In fact, even an algorithm that looks at two items at a time and takes _n_/2 steps has _O_(_n_). This is because, as _n_ gets bigger and bigger, only the largest term, _n_, matters.
*   Similarly, a logarithmic running time is _O_(log _n_), no matter what the base is, since this is just an approximation of what happens with _n_ is very large.
*   There are some common running times:
    *   _O_(_n_2)
    *   _O_(_n_ log _n_)
    *   _O_(_n_)
        *   (linear search)
    *   _O_(log _n_)
        *   (binary search)
    *   _O_(1)
*   Computer scientists might also use big Ω, big Omega notation, which is the lower bound of number of steps for our algorithm. (Big _O_ is the upper bound of number of steps, or the worst case, and typically what we care about more.) With linear search, for example, the worst case is _n_ steps, but the best case is 1 step since our item might happen to be the first item we check. The best case for binary search, too, is 1 since our item might be in the middle of the array.
*   And we have a similar set of the most common big Ω running times:
    *   Ω(_n_2)
    *   Ω(_n_ log _n_)
    *   Ω(_n_)
        *   (counting the number of items)
    *   Ω(log _n_)
    *   Ω(1)
        *   (linear search, binary search)
