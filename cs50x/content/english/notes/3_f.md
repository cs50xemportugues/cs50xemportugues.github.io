
Merge sort
----------

*   We can take the idea of recusion to sorting, with another algorithm called merge sort. The pseudocode might look like:
    
        If only one item
          Return
        Else
            Sort left half of items
            Sort right half of items
            Merge sorted halves
        
    
*   We’ll best see this in practice with an unsorted list:
    
        7 4 5 2 6 3 8 1
        
    
*   First, we’ll sort the left half (the first four elements):
    
        7 4 5 2 | 6 3 8 1
        – – – –
        
    
*   Well, to sort that, we need to sort the left half of the left half first:
    
        7 4 | 5 2 | 6 3 8 1
        – –
        
    
*   Now, we have just one item, `7`, in the left half, and one item, `4`, in the right half. So we’ll merge that together, by taking the smallest item from each list first:
    
        – – | 5 2 | 6 3 8 1
        4 7
        
    
*   And now we go back to the right half of the left half, and sort it:
    
        – – | – – | 6 3 8 1
        4 7 | 2 5
        
    
*   Now, both halves of the left half are sorted, so we can merge the two of them together. We look at the start of each list, and take `2` since it’s smaller than `4`. Then, we take `4`, since it’s now the smallest item at the front of both lists. Then, we take `5`, and finally, `7`, to get:
    
        – – – – | 6 3 8 1
        – – – –
        2 4 5 7
        
    
*   We now sort the right half the same way. First, the left half of the right half:
    
        – – – – | – – | 8 1
        – – – – | 3 6 |
        2 4 5 7
        
    
*   Then, the right half of the right half:
    
        – – – – | – – | – –
        – – – – | 3 6 | 1 8
        2 4 5 7
        
    
*   We can merge the right half together now:
    
        – – – – | – – – –
        – – – – | – – – –
        2 4 5 7 | 1 3 6 8
        
    
*   And finally, we can merge both halves of the whole list, following the same steps as before. Notice that we don’t need to check all the elements of each half to find the smallest, since we know that each half is already sorted. Instead, we just take the smallest element of the two at the start of each half:
    
        – – – – | – – – –
        – – – – | – – – –
        2 4 5 7 | – 3 6 8
        1
        
    
        – – – – | – – – –
        – – – – | – – – –
        – 4 5 7 | – 3 6 8
        1 2
        
    
        – – – – | – – – –
        – – – – | – – – –
        – 4 5 7 | – – 6 8
        1 2 3
        
    
        – – – – | – – – –
        – – – – | – – – –
        – – 5 7 | – – 6 8
        1 2 3 4
        
    
        – – – – | – – – –
        – – – – | – – – –
        – – – 7 | – – 6 8
        1 2 3 4   5
        
    
        – – – – | – – – –
        – – – – | – – – –
        – – – 7 | – – – 8
        1 2 3 4   5 6
        
    
        – – – – | – – – –
        – – – – | – – – –
        – – – – | – – – 8
        1 2 3 4   5 6 7
        
    
        – – – – | – – – –
        – – – – | – – – –
        – – – – | – – – –
        1 2 3 4   5 6 7 8
        
    
*   It took a lot of steps, but it actually took fewer steps than the other algorithms we’ve seen so far. We broke our list in half each time, until we were “sorting” eight lists with one element each:
    
        7 | 4 | 5 | 2 | 6 | 3 | 8 | 1
        4   7 | 2   5 | 3   6 | 1   8
        2   4   5   7 | 1   3   6   8
        1   2   3   4   5   6   7   8
        
    
*   Since our algorithm divided the problem in half each time, its running time is logarithmic with _O_(log _n_). And after we sorted each half (or half of a half), we needed to merge together all the elements, with _n_ steps since we had to look at each element once.
*   So our total running time is _O_(_n_ log _n_):
    *   _O_(_n_2)
        *   bubble sort, selection sort
    *   _O_(_n_ log _n_)
        *   merge sort
    *   _O_(_n_)
        *   linear search
    *   _O_(log _n_)
        *   binary search
    *   _O_(1)
*   Since log _n_ is greater than 1 but less than _n_, _n_ log _n_ is in between _n_ (times 1) and _n_2.
*   The best case, Ω, is still _n_ log _n_, since we still sort each half first and then merge them together:
    *   Ω(_n_2)
        *   selection sort
    *   Ω(_n_ log _n_)
        *   merge sort
    *   Ω(_n_)
        *   bubble sort
    *   Ω(log _n_)
    *   Ω(1)
        *   linear search, binary search
*   Finally, there is another notation, Θ, Theta, which we use to describe running times of algorithms if the upper bound and lower bound is the same. For example, merge sort has Θ(_n_ log _n_) since the best and worst case both require the same number of steps. And selection sort has Θ(_n_2).
*   We look at a [final visualization](https://www.youtube.com/watch?v=ZZuD6iUe3Pc) of sorting algorithms with a larger number of inputs, running at the same time.